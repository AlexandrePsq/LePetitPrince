{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate templates for LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from utils import check_folder, read_yaml, save_yaml, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_main = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/code/fMRI/main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dict = {'english': [57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
    "                    72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93,\n",
    "                    94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 113, 114, 115],\n",
    "                'french':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
    "                          21, 22, 23, 24, 25, 26, 27, 29, 30\n",
    "                         ]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_list = [\n",
    "    'spm', # hrf model used in SPM\n",
    "    'spm + derivative', # SPM model plus its time derivative (2 regressors)\n",
    "    'spm + derivative + dispersion', # idem, plus dispersion derivative (3 regressors)\n",
    "    'glover', # this one corresponds to the Glover hrf\n",
    "    'glover + derivative', # the Glover hrf + time derivative (2 regressors)\n",
    "    'glover + derivative + dispersion' # idem + dispersion derivative\n",
    "]\n",
    "hrf = 'spm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "temporal_shifting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "  # Shared General parameters\n",
    "  'subject': None,\n",
    "  'parallel': False,\n",
    "  'cuda': True,\n",
    "  'seed': 1111,\n",
    "  'language': None,\n",
    "  'path_to_root': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "  'path_to_fmridata': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI\",\n",
    "  'output': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/\"  ,\n",
    "  'input': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/\" ,\n",
    "  'detrend': True, \n",
    "  'standardize': True, \n",
    "  'high_pass': None, \n",
    "  'low_pass': None, \n",
    "  'mask_strategy': 'background', \n",
    "  #'dtype': 'float32', \n",
    "  'memory_level': 0, \n",
    "  'smoothing_fwhm': None , \n",
    "  'verbose': 0, \n",
    "\n",
    "\n",
    "  # Shared Splitter parameters\n",
    "  'nb_runs': 9,\n",
    "  'nb_runs_test': 1,\n",
    "\n",
    "  # Shared Compression parameters\n",
    "  'manifold_method': None,\n",
    "  'manifold_args': {'n_neighbors':4, 'random_state':1111, 'min_dist':0.0, 'metric':'cosine'},\n",
    "\n",
    "  # Shared Transformation parameters (includes the making of regressor and scaling)\n",
    "  'tr': 2.,\n",
    "  'scaling_mean': True,\n",
    "  'scaling_var': True,\n",
    "  'scaling_axis': 0,\n",
    "  'hrf': None,\n",
    "  'offset_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/onsets-offsets/\",\n",
    "  'duration_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "  'temporal_shifting': 0,\n",
    "  'oversampling': 10,\n",
    "  'add_noise_to_constant': True,\n",
    "\n",
    "  # Shared Estimator model parameters\n",
    "  'base': 10.0,\n",
    "  'voxel_wise': True,\n",
    "  'alpha_percentile': 99.9,\n",
    "  'alpha': None,\n",
    "  'alpha_min_log_scale': 1,\n",
    "  'alpha_max_log_scale': 5,\n",
    "  'nb_alphas': 10,\n",
    "  'optimizing_criteria': 'R2',\n",
    "  'estimator_model': 'Ridge()',\n",
    "  'save_all_weights': False, \n",
    "\n",
    "  # Maps creation parameters\n",
    "  'atlas': 'cort-prob-2mm',\n",
    "  'masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/ROI_masks/global_masker_english\",\n",
    "  'smoothed_masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/ROI_masks/smoothed_global_masker_english\",\n",
    "\n",
    "\n",
    "  # Models\n",
    "  'models': None, \n",
    "  'model_name': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_template(\n",
    "    model_name, \n",
    "    layers, \n",
    "    ninp,\n",
    "    nhid,\n",
    "    parameters,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type,\n",
    "    duration_type,\n",
    "    shift_surprisal,\n",
    "    includ_surprisal,\n",
    "    includ_entropy,\n",
    "    input_template='activations',\n",
    "    centering=False,\n",
    "    order=None,\n",
    "    scaling_type=None\n",
    "):\n",
    "    \n",
    "    columns_to_retrieve = []\n",
    "    for param in parameters:\n",
    "        columns_to_retrieve = ['{}-layer-{}-{}'.format(param, layer, i) for layer in layers for i in range(1, nhid + 1)]\n",
    "    if includ_surprisal:\n",
    "        columns_to_retrieve += ['surprisal']\n",
    "    if includ_entropy:\n",
    "        columns_to_retrieve += ['entropy']\n",
    "    result = { \n",
    "        'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal,\n",
    "        'input_template': input_template, # activations\n",
    "        'centering': centering,\n",
    "        'order': order,\n",
    "        'scaling_type': scaling_type,\n",
    "      }\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here starts the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/jobs.txt\"\n",
    "\n",
    "check_folder(os.path.join('/', job_to_launch_path.split('/')[1], '/'.join(templates_folder.split('/')[2:])))\n",
    "check_folder(os.path.join('/', job_to_launch_path.split('/')[1], '/'.join(sh_folder.split('/')[2:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_commands(command_lines, path_to_sh, job_to_launch_path, queue='Nspin_long'):\n",
    "    for index, command in enumerate(command_lines):\n",
    "        write(os.path.join('/', job_to_launch_path.split('/')[1], '/'.join(path_to_sh[index].split('/')[2:])), command)\n",
    "        walltime = '99:00:00'\n",
    "        job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "        write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l ncpus=2 -l walltime={walltime} {path_to_sh[index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "\n",
    "\"LSTM_embedding-size-768_nhid-768_nlayers-2_dropout-02_wikipedia_norm-2\",\n",
    "\"LSTM_embedding-size-768_nhid-768_nlayers-2_dropout-02_gutenberg_big_3_norm-2\",\n",
    "\"LSTM_embedding-size-768_nhid-768_nlayers-2_dropout-02_gutenberg_big_2_norm-2\",\n",
    "\"LSTM_embedding-size-768_nhid-768_nlayers-2_dropout-02_gutenberg_big_1_norm-2\",\n",
    "\"LSTM_embedding-size-768_nhid-300_nlayers-1_dropout-02_gutenberg_big_+_wiki_2_norm-2\",\n",
    "\n",
    "]\n",
    "hidden_layer_list = [[1, 2], [1, 2], [1, 2], [1, 2], [1]] * len(model_names)\n",
    "parameters_list = [['hidden']] #['hidden'], ['in'], ['forget'], ['out'], ['c_tilde'], ['cell']\n",
    "data_compression = [None] * len(model_names)\n",
    "ncomponents = [300] * len(model_names)\n",
    "shift_surprisal = False\n",
    "includ_surprisal = False\n",
    "includ_entropy = False\n",
    "params = [{'ninp': 768, 'nhid': 768, 'nlayers': 2}] * 4 + [{'ninp': 768, 'nhid': 300, 'nlayers': 1}] * 3\n",
    "order = [None] * len(model_names)\n",
    "centering = ['True'] * len(model_names)\n",
    "scaling_type = [None] * len(model_names)\n",
    "input_template = 'activations'\n",
    "scaling_axis = 0\n",
    "temporal_shifting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template['language'] = language\n",
    "template['scaling_axis'] = scaling_axis\n",
    "template['hrf'] = hrf\n",
    "template['temporal_shifting'] = temporal_shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_name in enumerate(model_names):\n",
    "    param = params[index]\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        for parameters in parameters_list:\n",
    "        \n",
    "            # hidden layers comparison\n",
    "            model = get_lstm_template(\n",
    "                model_name, \n",
    "                hidden_layer_list[index], \n",
    "                param['ninp'],\n",
    "                param['nhid'],\n",
    "                parameters,\n",
    "                \"{}_all-{}-layers\".format(model_name, parameters[0]),\n",
    "                data_compression[index], \n",
    "                ncomponents[index],\n",
    "                \"word+punctuation\",\n",
    "                None,\n",
    "                shift_surprisal,\n",
    "                includ_surprisal,\n",
    "                includ_entropy,\n",
    "                input_template='activations',\n",
    "                centering=centering[index],\n",
    "                order=order[index],\n",
    "                scaling_type=scaling_type[index]\n",
    "            )\n",
    "\n",
    "            additional = '_{}_{}'.format(data_compression[index], ncomponents[index]) if data_compression[index] is not None else ''\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_all-{}-layers{}'.format(model_name, subject, parameters[0], additional)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_all-{}-layers{}.yml'.format(model_name, subject, parameters[0], additional))\n",
    "            \n",
    "            save_yaml(template, os.path.join('/', job_to_launch_path.split('/')[1], '/'.join(yaml_path.split('/')[2:])))\n",
    "            #save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_all-{}-layers{}.sh'.format(model_name, subject, parameters[0], additional)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_commands(command_lines, path_to_sh, job_to_launch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 115,\n",
       " 'parallel': False,\n",
       " 'cuda': True,\n",
       " 'seed': 1111,\n",
       " 'language': 'english',\n",
       " 'path_to_root': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/',\n",
       " 'path_to_fmridata': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI',\n",
       " 'output': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/',\n",
       " 'input': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/',\n",
       " 'detrend': True,\n",
       " 'standardize': True,\n",
       " 'high_pass': None,\n",
       " 'low_pass': None,\n",
       " 'mask_strategy': 'background',\n",
       " 'memory_level': 0,\n",
       " 'smoothing_fwhm': None,\n",
       " 'verbose': 0,\n",
       " 'nb_runs': 9,\n",
       " 'nb_runs_test': 1,\n",
       " 'manifold_method': None,\n",
       " 'manifold_args': {'n_neighbors': 4,\n",
       "  'random_state': 1111,\n",
       "  'min_dist': 0.0,\n",
       "  'metric': 'cosine'},\n",
       " 'tr': 2.0,\n",
       " 'scaling_mean': True,\n",
       " 'scaling_var': True,\n",
       " 'scaling_axis': 0,\n",
       " 'hrf': 'spm',\n",
       " 'offset_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/onsets-offsets/',\n",
       " 'duration_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/',\n",
       " 'temporal_shifting': 0,\n",
       " 'oversampling': 10,\n",
       " 'add_noise_to_constant': True,\n",
       " 'base': 10.0,\n",
       " 'voxel_wise': True,\n",
       " 'alpha_percentile': 99.9,\n",
       " 'alpha': None,\n",
       " 'alpha_min_log_scale': 1,\n",
       " 'alpha_max_log_scale': 5,\n",
       " 'nb_alphas': 10,\n",
       " 'optimizing_criteria': 'R2',\n",
       " 'estimator_model': 'Ridge()',\n",
       " 'save_all_weights': False,\n",
       " 'atlas': 'cort-prob-2mm',\n",
       " 'masker_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/ROI_masks/global_masker_english',\n",
       " 'smoothed_masker_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/ROI_masks/smoothed_global_masker_english',\n",
       " 'models': [{'model_name': 'LSTM_embedding-size-768_nhid-300_nlayers-1_dropout-02_gutenberg_big_+_wiki_2_norm-2',\n",
       "   'columns_to_retrieve': \"['hidden-layer-1-1', 'hidden-layer-1-2', 'hidden-layer-1-3', 'hidden-layer-1-4', 'hidden-layer-1-5', 'hidden-layer-1-6', 'hidden-layer-1-7', 'hidden-layer-1-8', 'hidden-layer-1-9', 'hidden-layer-1-10', 'hidden-layer-1-11', 'hidden-layer-1-12', 'hidden-layer-1-13', 'hidden-layer-1-14', 'hidden-layer-1-15', 'hidden-layer-1-16', 'hidden-layer-1-17', 'hidden-layer-1-18', 'hidden-layer-1-19', 'hidden-layer-1-20', 'hidden-layer-1-21', 'hidden-layer-1-22', 'hidden-layer-1-23', 'hidden-layer-1-24', 'hidden-layer-1-25', 'hidden-layer-1-26', 'hidden-layer-1-27', 'hidden-layer-1-28', 'hidden-layer-1-29', 'hidden-layer-1-30', 'hidden-layer-1-31', 'hidden-layer-1-32', 'hidden-layer-1-33', 'hidden-layer-1-34', 'hidden-layer-1-35', 'hidden-layer-1-36', 'hidden-layer-1-37', 'hidden-layer-1-38', 'hidden-layer-1-39', 'hidden-layer-1-40', 'hidden-layer-1-41', 'hidden-layer-1-42', 'hidden-layer-1-43', 'hidden-layer-1-44', 'hidden-layer-1-45', 'hidden-layer-1-46', 'hidden-layer-1-47', 'hidden-layer-1-48', 'hidden-layer-1-49', 'hidden-layer-1-50', 'hidden-layer-1-51', 'hidden-layer-1-52', 'hidden-layer-1-53', 'hidden-layer-1-54', 'hidden-layer-1-55', 'hidden-layer-1-56', 'hidden-layer-1-57', 'hidden-layer-1-58', 'hidden-layer-1-59', 'hidden-layer-1-60', 'hidden-layer-1-61', 'hidden-layer-1-62', 'hidden-layer-1-63', 'hidden-layer-1-64', 'hidden-layer-1-65', 'hidden-layer-1-66', 'hidden-layer-1-67', 'hidden-layer-1-68', 'hidden-layer-1-69', 'hidden-layer-1-70', 'hidden-layer-1-71', 'hidden-layer-1-72', 'hidden-layer-1-73', 'hidden-layer-1-74', 'hidden-layer-1-75', 'hidden-layer-1-76', 'hidden-layer-1-77', 'hidden-layer-1-78', 'hidden-layer-1-79', 'hidden-layer-1-80', 'hidden-layer-1-81', 'hidden-layer-1-82', 'hidden-layer-1-83', 'hidden-layer-1-84', 'hidden-layer-1-85', 'hidden-layer-1-86', 'hidden-layer-1-87', 'hidden-layer-1-88', 'hidden-layer-1-89', 'hidden-layer-1-90', 'hidden-layer-1-91', 'hidden-layer-1-92', 'hidden-layer-1-93', 'hidden-layer-1-94', 'hidden-layer-1-95', 'hidden-layer-1-96', 'hidden-layer-1-97', 'hidden-layer-1-98', 'hidden-layer-1-99', 'hidden-layer-1-100', 'hidden-layer-1-101', 'hidden-layer-1-102', 'hidden-layer-1-103', 'hidden-layer-1-104', 'hidden-layer-1-105', 'hidden-layer-1-106', 'hidden-layer-1-107', 'hidden-layer-1-108', 'hidden-layer-1-109', 'hidden-layer-1-110', 'hidden-layer-1-111', 'hidden-layer-1-112', 'hidden-layer-1-113', 'hidden-layer-1-114', 'hidden-layer-1-115', 'hidden-layer-1-116', 'hidden-layer-1-117', 'hidden-layer-1-118', 'hidden-layer-1-119', 'hidden-layer-1-120', 'hidden-layer-1-121', 'hidden-layer-1-122', 'hidden-layer-1-123', 'hidden-layer-1-124', 'hidden-layer-1-125', 'hidden-layer-1-126', 'hidden-layer-1-127', 'hidden-layer-1-128', 'hidden-layer-1-129', 'hidden-layer-1-130', 'hidden-layer-1-131', 'hidden-layer-1-132', 'hidden-layer-1-133', 'hidden-layer-1-134', 'hidden-layer-1-135', 'hidden-layer-1-136', 'hidden-layer-1-137', 'hidden-layer-1-138', 'hidden-layer-1-139', 'hidden-layer-1-140', 'hidden-layer-1-141', 'hidden-layer-1-142', 'hidden-layer-1-143', 'hidden-layer-1-144', 'hidden-layer-1-145', 'hidden-layer-1-146', 'hidden-layer-1-147', 'hidden-layer-1-148', 'hidden-layer-1-149', 'hidden-layer-1-150', 'hidden-layer-1-151', 'hidden-layer-1-152', 'hidden-layer-1-153', 'hidden-layer-1-154', 'hidden-layer-1-155', 'hidden-layer-1-156', 'hidden-layer-1-157', 'hidden-layer-1-158', 'hidden-layer-1-159', 'hidden-layer-1-160', 'hidden-layer-1-161', 'hidden-layer-1-162', 'hidden-layer-1-163', 'hidden-layer-1-164', 'hidden-layer-1-165', 'hidden-layer-1-166', 'hidden-layer-1-167', 'hidden-layer-1-168', 'hidden-layer-1-169', 'hidden-layer-1-170', 'hidden-layer-1-171', 'hidden-layer-1-172', 'hidden-layer-1-173', 'hidden-layer-1-174', 'hidden-layer-1-175', 'hidden-layer-1-176', 'hidden-layer-1-177', 'hidden-layer-1-178', 'hidden-layer-1-179', 'hidden-layer-1-180', 'hidden-layer-1-181', 'hidden-layer-1-182', 'hidden-layer-1-183', 'hidden-layer-1-184', 'hidden-layer-1-185', 'hidden-layer-1-186', 'hidden-layer-1-187', 'hidden-layer-1-188', 'hidden-layer-1-189', 'hidden-layer-1-190', 'hidden-layer-1-191', 'hidden-layer-1-192', 'hidden-layer-1-193', 'hidden-layer-1-194', 'hidden-layer-1-195', 'hidden-layer-1-196', 'hidden-layer-1-197', 'hidden-layer-1-198', 'hidden-layer-1-199', 'hidden-layer-1-200', 'hidden-layer-1-201', 'hidden-layer-1-202', 'hidden-layer-1-203', 'hidden-layer-1-204', 'hidden-layer-1-205', 'hidden-layer-1-206', 'hidden-layer-1-207', 'hidden-layer-1-208', 'hidden-layer-1-209', 'hidden-layer-1-210', 'hidden-layer-1-211', 'hidden-layer-1-212', 'hidden-layer-1-213', 'hidden-layer-1-214', 'hidden-layer-1-215', 'hidden-layer-1-216', 'hidden-layer-1-217', 'hidden-layer-1-218', 'hidden-layer-1-219', 'hidden-layer-1-220', 'hidden-layer-1-221', 'hidden-layer-1-222', 'hidden-layer-1-223', 'hidden-layer-1-224', 'hidden-layer-1-225', 'hidden-layer-1-226', 'hidden-layer-1-227', 'hidden-layer-1-228', 'hidden-layer-1-229', 'hidden-layer-1-230', 'hidden-layer-1-231', 'hidden-layer-1-232', 'hidden-layer-1-233', 'hidden-layer-1-234', 'hidden-layer-1-235', 'hidden-layer-1-236', 'hidden-layer-1-237', 'hidden-layer-1-238', 'hidden-layer-1-239', 'hidden-layer-1-240', 'hidden-layer-1-241', 'hidden-layer-1-242', 'hidden-layer-1-243', 'hidden-layer-1-244', 'hidden-layer-1-245', 'hidden-layer-1-246', 'hidden-layer-1-247', 'hidden-layer-1-248', 'hidden-layer-1-249', 'hidden-layer-1-250', 'hidden-layer-1-251', 'hidden-layer-1-252', 'hidden-layer-1-253', 'hidden-layer-1-254', 'hidden-layer-1-255', 'hidden-layer-1-256', 'hidden-layer-1-257', 'hidden-layer-1-258', 'hidden-layer-1-259', 'hidden-layer-1-260', 'hidden-layer-1-261', 'hidden-layer-1-262', 'hidden-layer-1-263', 'hidden-layer-1-264', 'hidden-layer-1-265', 'hidden-layer-1-266', 'hidden-layer-1-267', 'hidden-layer-1-268', 'hidden-layer-1-269', 'hidden-layer-1-270', 'hidden-layer-1-271', 'hidden-layer-1-272', 'hidden-layer-1-273', 'hidden-layer-1-274', 'hidden-layer-1-275', 'hidden-layer-1-276', 'hidden-layer-1-277', 'hidden-layer-1-278', 'hidden-layer-1-279', 'hidden-layer-1-280', 'hidden-layer-1-281', 'hidden-layer-1-282', 'hidden-layer-1-283', 'hidden-layer-1-284', 'hidden-layer-1-285', 'hidden-layer-1-286', 'hidden-layer-1-287', 'hidden-layer-1-288', 'hidden-layer-1-289', 'hidden-layer-1-290', 'hidden-layer-1-291', 'hidden-layer-1-292', 'hidden-layer-1-293', 'hidden-layer-1-294', 'hidden-layer-1-295', 'hidden-layer-1-296', 'hidden-layer-1-297', 'hidden-layer-1-298', 'hidden-layer-1-299', 'hidden-layer-1-300']\",\n",
       "   'surname': 'LSTM_embedding-size-768_nhid-300_nlayers-1_dropout-02_gutenberg_big_+_wiki_2_norm-2_all-hidden-layers',\n",
       "   'data_compression': None,\n",
       "   'ncomponents': 300,\n",
       "   'offset_type': 'word+punctuation',\n",
       "   'duration_type': None,\n",
       "   'shift_surprisal': False,\n",
       "   'input_template': 'activations',\n",
       "   'centering': 'True',\n",
       "   'order': None,\n",
       "   'scaling_type': None}],\n",
       " 'model_name': 'LSTM_embedding-size-768_nhid-300_nlayers-1_dropout-02_gutenberg_big_+_wiki_2_norm-2_115_all-hidden-layers'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(command_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
