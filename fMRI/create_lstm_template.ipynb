{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate templates for LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from utils import check_folder, read_yaml, save_yaml, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_main = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/code/fMRI/main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dict = {'english': [57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
    "                    72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93,\n",
    "                    94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 113, 114, 115],\n",
    "                'french':[1, 2, 3, 4, ,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
    "                          21, 22, 23, 24, 25, 26, 27, 29, 30\n",
    "                         ]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_list = [\n",
    "    'spm', # hrf model used in SPM\n",
    "    'spm + derivative', # SPM model plus its time derivative (2 regressors)\n",
    "    'spm + derivative + dispersion', # idem, plus dispersion derivative (3 regressors)\n",
    "    'glover', # this one corresponds to the Glover hrf\n",
    "    'glover + derivative', # the Glover hrf + time derivative (2 regressors)\n",
    "    'glover + derivative + dispersion' # idem + dispersion derivative\n",
    "]\n",
    "hrf = 'spm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "  # Shared General parameters\n",
    "  'subject': None,\n",
    "  'parallel': False,\n",
    "  'cuda': True,\n",
    "  'seed': 1111,\n",
    "  'language': None,\n",
    "  'path_to_root': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "  'path_to_fmridata': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI\",\n",
    "  'output': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/\"  ,\n",
    "  'input': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/\" ,\n",
    "  'detrend': True, \n",
    "  'standardize': True, \n",
    "  'high_pass': None, \n",
    "  'low_pass': None, \n",
    "  'mask_strategy': 'background', \n",
    "  #'dtype': 'float32', \n",
    "  'memory_level': 0, \n",
    "  'smoothing_fwhm': None , \n",
    "  'verbose': 0, \n",
    "    \n",
    "  # Shared Splitter parameters\n",
    "  'nb_runs': 9,\n",
    "  'nb_runs_test': 1,\n",
    "\n",
    "  # Shared Compression parameters\n",
    "\n",
    "\n",
    "  # Shared Transformation parameters (includes the making of regressor and scaling)\n",
    "  'tr': 2.,\n",
    "  'scaling_mean': True,\n",
    "  'scaling_var': True,\n",
    "  'scaling_axis': None,\n",
    "  'hrf': None,\n",
    "  'offset_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/onsets-offsets/\",\n",
    "  'duration_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "\n",
    "  # Shared Estimator model parameters\n",
    "  'base': 10.0,\n",
    "  'voxel_wise': True,\n",
    "  'alpha_percentile': 99.9,\n",
    "  'alpha': None,\n",
    "  'alpha_min_log_scale': 2,\n",
    "  'alpha_max_log_scale': 5,\n",
    "  'nb_alphas': 10,\n",
    "  'optimizing_criteria': 'R2',\n",
    "  'estimator_model': 'Ridge()',\n",
    "\n",
    "  # Maps creation parameters\n",
    "  'atlas': 'cort-prob-2mm',\n",
    "  'masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english\",\n",
    "  'smoothed_masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english\",\n",
    "\n",
    "\n",
    "  # Models\n",
    "  'models': None, \n",
    "  'model_name': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_commands(command_lines, path_to_sh):\n",
    "    for index, command in enumerate(command_lines):\n",
    "        write(path_to_sh[index], command)\n",
    "        queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "        walltime = '99:00:00'\n",
    "        output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "        error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "        job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "        write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_template(\n",
    "    model_name, \n",
    "    layers, \n",
    "    ninp,\n",
    "    nhid,\n",
    "    parameters,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type,\n",
    "    duration_type,\n",
    "    shift_surprisal,\n",
    "    includ_surprisal,\n",
    "    includ_entropy,\n",
    "    input_template='activations'):\n",
    "    \n",
    "    columns_to_retrieve = []\n",
    "    for param in parameters:\n",
    "        columns_to_retrieve = ['{}-layer-{}-{}'.format(param, layer, i) for layer in layers for i in range(1, nhid + 1)]\n",
    "    if includ_surprisal:\n",
    "        columns_to_retrieve += ['surprisal']\n",
    "    if includ_entropy:\n",
    "        columns_to_retrieve += ['entropy']\n",
    "    result = { 'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal,\n",
    "        'input_template': input_template # activations\n",
    "      }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here starts the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/jobs.txt\"\n",
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['LSTM_embedding-size_650_nhid_650_nlayers_2_dropout_02_wiki_kristina_english',\n",
    " 'LSTM_embedding-size_600_nhid_300_nlayers_1_dropout_02_wiki_kristina_english']\n",
    "hidden_layer_list = [[i for i in range(1, 3)], [1]]\n",
    "parameters_list = [['in'], ['forget'], ['out'], ['c_tilde'], ['cell']] #['hidden'],\n",
    "data_compression = ['pca', None]\n",
    "ncomponents = [300, None]\n",
    "shift_surprisal = False\n",
    "includ_surprisal = False\n",
    "includ_entropy = False\n",
    "params = [{'ninp': 650, 'nhid': 650, 'nlayers': 2}, {'ninp': 600, 'nhid': 300, 'nlayers': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_name in enumerate(model_names):\n",
    "    param = params[index]\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        for parameters in parameters_list:\n",
    "        \n",
    "            # hidden layers comparison\n",
    "            model = get_lstm_template(model_name, \n",
    "                                        hidden_layer_list[index], \n",
    "                                        param['ninp'],\n",
    "                                        param['nhid'],\n",
    "                                        parameters,\n",
    "                                        \"{}_all-{}-layers\".format(model_name, parameters[0]),\n",
    "                                        data_compression[index], \n",
    "                                        ncomponents[index],\n",
    "                                        \"word+punctuation\",\n",
    "                                        None,\n",
    "                                        shift_surprisal,\n",
    "                                        includ_surprisal,\n",
    "                                        includ_entropy,\n",
    "                                        input_template='activations')\n",
    "\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_all-{}-layers'.format(model_name, subject, parameters[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_all-{}-layers.yml'.format(model_name, subject, parameters[0]))\n",
    "\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_all-{}-layers.sh'.format(model_name, subject, parameters[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
