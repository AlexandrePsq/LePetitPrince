{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate templates for Transformer-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from utils import check_folder, read_yaml, save_yaml, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_main = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/code/fMRI/main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dict = {'english': [57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
    "                    72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93,\n",
    "                    94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 113, 114, 115],\n",
    "                'french':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
    "                          21, 22, 23, 24, 25, 26, 27, 29, 30\n",
    "                         ]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_list = [\n",
    "    'spm', # hrf model used in SPM\n",
    "    'spm + derivative', # SPM model plus its time derivative (2 regressors)\n",
    "    'spm + derivative + dispersion', # idem, plus dispersion derivative (3 regressors)\n",
    "    'glover', # this one corresponds to the Glover hrf\n",
    "    'glover + derivative', # the Glover hrf + time derivative (2 regressors)\n",
    "    'glover + derivative + dispersion' # idem + dispersion derivative\n",
    "]\n",
    "hrf = 'spm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "temporal_shifting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "  # Shared General parameters\n",
    "  'subject': None,\n",
    "  'parallel': False,\n",
    "  'cuda': True,\n",
    "  'seed': 1111,\n",
    "  'language': None,\n",
    "  'path_to_root': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "  'path_to_fmridata': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI\",\n",
    "  'output': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/\"  ,\n",
    "  'input': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/\" ,\n",
    "  'detrend': True, \n",
    "  'standardize': True, \n",
    "  'high_pass': None, \n",
    "  'low_pass': None, \n",
    "  'mask_strategy': 'background', \n",
    "  #'dtype': 'float32', \n",
    "  'memory_level': 0, \n",
    "  'smoothing_fwhm': None , \n",
    "  'verbose': 0, \n",
    "\n",
    "\n",
    "  # Shared Splitter parameters\n",
    "  'nb_runs': 9,\n",
    "  'nb_runs_test': 1,\n",
    "\n",
    "  # Shared Compression parameters\n",
    "  'manifold_method': None,\n",
    "  'manifold_args': {'n_neighbors':4, 'random_state':1111, 'min_dist':0.0, 'metric':'cosine'},\n",
    "\n",
    "  # Shared Transformation parameters (includes the making of regressor and scaling)\n",
    "  'tr': 2.,\n",
    "  'scaling_mean': True,\n",
    "  'scaling_var': True,\n",
    "  'scaling_axis': None,\n",
    "  'hrf': None,\n",
    "  'offset_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/onsets-offsets/\",\n",
    "  'duration_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "  'temporal_shifting': 0,\n",
    "\n",
    "  # Shared Estimator model parameters\n",
    "  'base': 10.0,\n",
    "  'voxel_wise': True,\n",
    "  'alpha_percentile': 99.9,\n",
    "  'alpha': None,\n",
    "  'alpha_min_log_scale': 2,\n",
    "  'alpha_max_log_scale': 5,\n",
    "  'nb_alphas': 10,\n",
    "  'optimizing_criteria': 'R2',\n",
    "  'estimator_model': 'Ridge()',\n",
    "\n",
    "  # Maps creation parameters\n",
    "  'atlas': 'cort-prob-2mm',\n",
    "  'masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english\",\n",
    "  'smoothed_masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english\",\n",
    "\n",
    "\n",
    "  # Models\n",
    "  'models': None, \n",
    "  'model_name': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_commands(command_lines, path_to_sh, queue='Nspin_long'):\n",
    "    for index, command in enumerate(command_lines):\n",
    "        write(path_to_sh[index], command)\n",
    "        queue = queue # 'Nspin_bigM'\n",
    "        walltime = '99:00:00'\n",
    "        output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "        error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "        job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "        write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_template(\n",
    "    model_name, \n",
    "    layers, \n",
    "    hidden_states, \n",
    "    attention_heads, \n",
    "    heads,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type='word+punctuation',\n",
    "    duration_type=None,\n",
    "    centering=False,\n",
    "    shift_surprisal=False,\n",
    "    scaling_type=None,\n",
    "    order=None,\n",
    "    input_template='activations'):\n",
    "    \n",
    "    columns_to_retrieve = []\n",
    "    if hidden_states:\n",
    "        columns_to_retrieve = ['hidden_state-layer-{}-{}'.format(layer, i) for layer in layers for i in range(1, 769)]\n",
    "    if attention_heads:\n",
    "        columns_to_retrieve += ['attention-layer-{}-head-{}-{}'.format(layer, head, i) for layer in layers for head in heads for i in range(1, 65)]\n",
    "    result = { \n",
    "        'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal,\n",
    "        'centering': centering,\n",
    "        'order': order,\n",
    "        'scaling_type': scaling_type,\n",
    "        'input_template': input_template # cls / sep / activations\n",
    "      }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template_folder(\n",
    "    model_names, \n",
    "    language, \n",
    "    template, \n",
    "    hidden_layer_list,\n",
    "    attention_layer_list,\n",
    "    attention_layer_head_list,\n",
    "    centering,\n",
    "    order,\n",
    "    scaling_type,\n",
    "    input_template,\n",
    "    data_compression,\n",
    "    ncomponents,\n",
    "    temporal_shifting,\n",
    "    path_to_main=path_to_main):\n",
    "    command_lines = []\n",
    "    path_to_sh = []\n",
    "    for index, model_name in enumerate(model_names):\n",
    "        for subject in subject_dict[language]:\n",
    "            template['subject'] = subject\n",
    "\n",
    "            # hidden layers comparison\n",
    "            for hidden_layers in hidden_layer_list:\n",
    "                model = get_model_template(model_name=model_name, \n",
    "                                           layers=hidden_layers, \n",
    "                                           hidden_states=True,\n",
    "                                           attention_heads=False, \n",
    "                                           heads=None, \n",
    "                                           surname=\"{}_hidden-layer-{}\".format(model_name, hidden_layers),\n",
    "                                           data_compression=data_compression[index], \n",
    "                                           ncomponents=ncomponents[index],\n",
    "                                           offset_type=\"word+punctuation\", \n",
    "                                           duration_type=None, \n",
    "                                           centering=centering[index],\n",
    "                                           order=order[index],\n",
    "                                           shift_surprisal=False,\n",
    "                                           scaling_type=scaling_type[index],\n",
    "                                           input_template=input_template\n",
    "                                          )\n",
    "                template['models'] = [model]\n",
    "                additional = '_{}_{}'.format(data_compression[index], ncomponents[index]) if data_compression[index] is not None else ''\n",
    "                #template['model_name'] = '{}_norm-{}_temporal-shifting-{}_{}_hidden-layer-{}'.format(model_name, order[index], temporal_shifting, subject, hidden_layers[0]).replace('np.', '')\n",
    "                #yaml_path = os.path.join(templates_folder, '{}_norm-{}_temporal-shifting-{}_{}_hidden-layer-{}.yml'.format(model_name,  order[index], temporal_shifting, subject, hidden_layers[0])).replace('np.', '')\n",
    "                template['model_name'] = '{}_norm-{}_temporal-shifting-{}_{}_hidden-all-layers{}'.format(model_name, order[index], temporal_shifting, subject, additional).replace('np.', '')\n",
    "                yaml_path = os.path.join(templates_folder, '{}_norm-{}_temporal-shifting-{}_{}_hidden-all-layers{}.yml'.format(model_name, order[index], temporal_shifting, subject, additional)).replace('np.', '')\n",
    "\n",
    "                save_yaml(template, yaml_path)\n",
    "                command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "                #path_to_sh.append(os.path.join(sh_folder, '{}_norm-{}_temporal-shifting-{}_{}_hidden-layer-{}.sh'.format(model_name, order[index], temporal_shifting, subject, hidden_layers[0]).replace('np.', '')))\n",
    "                path_to_sh.append(os.path.join(sh_folder, '{}_norm-{}_temporal-shifting-{}_{}_hidden-all-layers{}.sh'.format(model_name, order[index], temporal_shifting, subject, additional).replace('np.', '')))\n",
    "\n",
    "            # attention layers comparison\n",
    "            for attention_layers in attention_layer_list:\n",
    "                model = get_model_template(model_name=model_name, \n",
    "                                           layers=attention_layers, \n",
    "                                           hidden_states=False,\n",
    "                                           attention_heads=True, \n",
    "                                           heads=heads, \n",
    "                                           surname=\"{}_norm-{}_attention-layer-{}\".format(model_name, order[index], attention_layers),\n",
    "                                           data_compression=data_compression[index], \n",
    "                                           ncomponents=ncomponents[index],\n",
    "                                           offset_type=\"word+punctuation\", \n",
    "                                           duration_type=None, \n",
    "                                           centering=centering[index],\n",
    "                                           order=order[index],\n",
    "                                           shift_surprisal=False,\n",
    "                                           scaling_type=scaling_type[index],\n",
    "                                           input_template=input_template\n",
    "                                          )\n",
    "                template['models'] = [model]\n",
    "                #additional = '_{}_{}'.format(data_compression[index], ncomponents[index]) if data_compression[index] is not None else ''\n",
    "                template['model_name'] = '{}_norm-{}_{}_attention_layer-{}'.format(model_name, order[index], subject, attention_layers[0])\n",
    "                yaml_path = os.path.join(templates_folder, '{}_norm-{}_{}_attention-layer-{}.yml'.format(model_name, order[index], subject, attention_layers[0]))\n",
    "                save_yaml(template, yaml_path)\n",
    "                command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "                path_to_sh.append(os.path.join(sh_folder, '{}_norm-{}_{}_attention-layer-{}.sh'.format(model_name, order[index], subject, attention_layers[0])))\n",
    "\n",
    "            # specific attention heads comparison    \n",
    "            for (layer, head) in attention_layer_head_list:\n",
    "                model = get_model_template(model_name=model_name, \n",
    "                                           layers=[layer], \n",
    "                                           hidden_states=False,\n",
    "                                           attention_heads=True, \n",
    "                                           heads=[head], \n",
    "                                           surname=\"{}_norm-{}_attention-layer-{}-head-{}\".format(model_name, order[index], layer, head),\n",
    "                                           data_compression=None, \n",
    "                                           ncomponents=None,\n",
    "                                           offset_type=\"word+punctuation\", \n",
    "                                           duration_type=None, \n",
    "                                           centering=centering[index],\n",
    "                                           order=order[index],\n",
    "                                           shift_surprisal=False,\n",
    "                                           scaling_type=scaling_type[index],\n",
    "                                           input_template=input_template\n",
    "                                          )\n",
    "                template['models'] = [model]\n",
    "                template['model_name'] = '{}_norm-{}_temporal-shifting-{}_{}_attention-layer-{}_head-{}'.format(model_name, order[index], temporal_shifting, subject, layer, head)\n",
    "                yaml_path = os.path.join(templates_folder, '{}_norm-{}_temporal-shifting-{}_{}_attention-layer-{}_head-{}.yml'.format(model_name, order[index], temporal_shifting, subject, layer, head))\n",
    "                save_yaml(template, yaml_path)\n",
    "                command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "                path_to_sh.append(os.path.join(sh_folder, '{}_norm-{}_temporal-shifting-{}_{}_attention-layer-{}_head-{}.sh'.format(model_name, order[index], temporal_shifting, subject, layer, head)))\n",
    "    return path_to_sh, command_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we start the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_transformers_0/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_transformers_0/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_transformers_0/jobs.txt\"\n",
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'bert-base-cased_pre-7_1_post-0_norm-None',\n",
    "    'gpt2_pre-20_1_norm-inf',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_names = ['bert-base-cased', 'gpt2_scaled', 'roberta-base']\n",
    "hidden_layer_list = [[i] for i in range(13)]\n",
    "attention_layer_list = [] #[[i] for i in range(1, 13)]\n",
    "heads = np.arange(1, 13)\n",
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]\n",
    "command_lines = []\n",
    "data_compression = [None, None]\n",
    "ncomponents = [None, None]\n",
    "order = ['np.inf'] * 2\n",
    "centering = ['True'] * 2 \n",
    "scaling_type = ['normalize'] * 2\n",
    "input_template = 'activations'\n",
    "scaling_axis = 1\n",
    "temporal_shifting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "template['scaling_axis'] = scaling_axis\n",
    "template['language'] = language\n",
    "template['temporal_shifting'] = temporal_shifting\n",
    "template['hrf'] = hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sh, command_lines = fill_template_folder(\n",
    "                    model_names, \n",
    "                    language, \n",
    "                    template, \n",
    "                    hidden_layer_list,\n",
    "                    attention_layer_list,\n",
    "                    attention_layer_head_list,\n",
    "                    centering,\n",
    "                    order,\n",
    "                    scaling_type,\n",
    "                    input_template,\n",
    "                    data_compression,\n",
    "                    ncomponents,\n",
    "                    temporal_shifting,\n",
    "                    path_to_main=path_to_main\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_commands(command_lines, path_to_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 115,\n",
       " 'parallel': False,\n",
       " 'cuda': True,\n",
       " 'seed': 1111,\n",
       " 'language': 'english',\n",
       " 'path_to_root': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/',\n",
       " 'path_to_fmridata': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI',\n",
       " 'output': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/',\n",
       " 'input': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/',\n",
       " 'detrend': True,\n",
       " 'standardize': True,\n",
       " 'high_pass': None,\n",
       " 'low_pass': None,\n",
       " 'mask_strategy': 'background',\n",
       " 'memory_level': 0,\n",
       " 'smoothing_fwhm': None,\n",
       " 'verbose': 0,\n",
       " 'nb_runs': 9,\n",
       " 'nb_runs_test': 1,\n",
       " 'tr': 2.0,\n",
       " 'scaling_mean': True,\n",
       " 'scaling_var': True,\n",
       " 'scaling_axis': 1,\n",
       " 'hrf': 'spm',\n",
       " 'offset_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/onsets-offsets/',\n",
       " 'duration_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/',\n",
       " 'temporal_shifting': 0,\n",
       " 'base': 10.0,\n",
       " 'voxel_wise': True,\n",
       " 'alpha_percentile': 99.9,\n",
       " 'alpha': None,\n",
       " 'alpha_min_log_scale': 2,\n",
       " 'alpha_max_log_scale': 5,\n",
       " 'nb_alphas': 10,\n",
       " 'optimizing_criteria': 'R2',\n",
       " 'estimator_model': 'Ridge()',\n",
       " 'atlas': 'cort-prob-2mm',\n",
       " 'masker_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english',\n",
       " 'smoothed_masker_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english',\n",
       " 'models': [{'model_name': 'gpt2_pre-20_1_norm-inf',\n",
       "   'columns_to_retrieve': \"['attention-layer-9-head-6-1', 'attention-layer-9-head-6-2', 'attention-layer-9-head-6-3', 'attention-layer-9-head-6-4', 'attention-layer-9-head-6-5', 'attention-layer-9-head-6-6', 'attention-layer-9-head-6-7', 'attention-layer-9-head-6-8', 'attention-layer-9-head-6-9', 'attention-layer-9-head-6-10', 'attention-layer-9-head-6-11', 'attention-layer-9-head-6-12', 'attention-layer-9-head-6-13', 'attention-layer-9-head-6-14', 'attention-layer-9-head-6-15', 'attention-layer-9-head-6-16', 'attention-layer-9-head-6-17', 'attention-layer-9-head-6-18', 'attention-layer-9-head-6-19', 'attention-layer-9-head-6-20', 'attention-layer-9-head-6-21', 'attention-layer-9-head-6-22', 'attention-layer-9-head-6-23', 'attention-layer-9-head-6-24', 'attention-layer-9-head-6-25', 'attention-layer-9-head-6-26', 'attention-layer-9-head-6-27', 'attention-layer-9-head-6-28', 'attention-layer-9-head-6-29', 'attention-layer-9-head-6-30', 'attention-layer-9-head-6-31', 'attention-layer-9-head-6-32', 'attention-layer-9-head-6-33', 'attention-layer-9-head-6-34', 'attention-layer-9-head-6-35', 'attention-layer-9-head-6-36', 'attention-layer-9-head-6-37', 'attention-layer-9-head-6-38', 'attention-layer-9-head-6-39', 'attention-layer-9-head-6-40', 'attention-layer-9-head-6-41', 'attention-layer-9-head-6-42', 'attention-layer-9-head-6-43', 'attention-layer-9-head-6-44', 'attention-layer-9-head-6-45', 'attention-layer-9-head-6-46', 'attention-layer-9-head-6-47', 'attention-layer-9-head-6-48', 'attention-layer-9-head-6-49', 'attention-layer-9-head-6-50', 'attention-layer-9-head-6-51', 'attention-layer-9-head-6-52', 'attention-layer-9-head-6-53', 'attention-layer-9-head-6-54', 'attention-layer-9-head-6-55', 'attention-layer-9-head-6-56', 'attention-layer-9-head-6-57', 'attention-layer-9-head-6-58', 'attention-layer-9-head-6-59', 'attention-layer-9-head-6-60', 'attention-layer-9-head-6-61', 'attention-layer-9-head-6-62', 'attention-layer-9-head-6-63', 'attention-layer-9-head-6-64']\",\n",
       "   'surname': 'gpt2_pre-20_1_norm-inf_norm-np.inf_attention-layer-9-head-6',\n",
       "   'data_compression': None,\n",
       "   'ncomponents': None,\n",
       "   'offset_type': 'word+punctuation',\n",
       "   'duration_type': None,\n",
       "   'shift_surprisal': False,\n",
       "   'centering': 'True',\n",
       "   'order': 'np.inf',\n",
       "   'scaling_type': 'normalize',\n",
       "   'input_template': 'activations'}],\n",
       " 'model_name': 'gpt2_pre-20_1_norm-inf_norm-np.inf_temporal-shifting-0_115_attention-layer-9_head-6'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'bert-base-cased_pre-7_1_post-0_norm-None',\n",
    "    #'gpt2_pre-20_1_norm-inf',\n",
    "    #'bert-base-cased_pre-7_1_post-0_norm-None',\n",
    "    #'gpt2_pre-20_1_norm-inf',\n",
    "] *30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_list = [[i for i in range(13)]]\n",
    "attention_layer_list = [] # [[i for i in range(1, 13)]]\n",
    "attention_layer_head_list =  [] # np.arange(1, 13)\n",
    "command_lines = []\n",
    "data_compression = ['pca'] * 14 + ['umap'] * 16\n",
    "ncomponents = [10, 25, 50, 100, 150, 500, 1000] * 2 + [10, 25, 50, 100, 150, 300, 500, 1000] * 2\n",
    "#order = ['std'] * 8 + ['None'] * 8 + ['np.inf'] * 8 + ['5'] * 8 + ['3'] * 8 + ['2'] * 8 # to replace with only None\n",
    "order = ['np.inf'] * 7 + ['2'] * 7 + ['np.inf'] * 8 + ['2'] * 8   # to replace with only None\n",
    "centering = ['True'] * 30\n",
    "#scaling_type = ['standardize'] * 8 + ['normalize'] * 40\n",
    "scaling_type = ['normalize'] * 30\n",
    "input_template = 'activations'\n",
    "scaling_axis = 1\n",
    "temporal_shifting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for index, model_name in enumerate(model_names):\n",
    "#    additional = '_{}_{}'.format(data_compression[index], ncomponents[index]) if data_compression[index] is not None else ''\n",
    "#    print('{}_norm-{}_{}_hidden-all-layers{}'.format(model_name, order[index], '{}',additional).replace('np.', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template['scaling_axis'] = scaling_axis\n",
    "template['language'] = language\n",
    "template['temporal_shifting'] = temporal_shifting\n",
    "template['hrf'] = hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_sh, command_lines = fill_template_folder(\n",
    "                    model_names, \n",
    "                    language, \n",
    "                    template, \n",
    "                    hidden_layer_list,\n",
    "                    attention_layer_list,\n",
    "                    attention_layer_head_list,\n",
    "                    centering,\n",
    "                    order,\n",
    "                    scaling_type,\n",
    "                    input_template,\n",
    "                    data_compression,\n",
    "                    ncomponents,\n",
    "                    temporal_shifting, \n",
    "                    path_to_main=path_to_main\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_commands(command_lines, path_to_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': None,\n",
       " 'parallel': False,\n",
       " 'cuda': True,\n",
       " 'seed': 1111,\n",
       " 'language': 'english',\n",
       " 'path_to_root': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/',\n",
       " 'path_to_fmridata': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI',\n",
       " 'output': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/',\n",
       " 'input': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/',\n",
       " 'detrend': True,\n",
       " 'standardize': True,\n",
       " 'high_pass': None,\n",
       " 'low_pass': None,\n",
       " 'mask_strategy': 'background',\n",
       " 'memory_level': 0,\n",
       " 'smoothing_fwhm': None,\n",
       " 'verbose': 0,\n",
       " 'nb_runs': 9,\n",
       " 'nb_runs_test': 1,\n",
       " 'manifold_method': None,\n",
       " 'manifold_args': {'n_neighbors': 4,\n",
       "  'random_state': 1111,\n",
       "  'min_dist': 0.0,\n",
       "  'metric': 'cosine'},\n",
       " 'tr': 2.0,\n",
       " 'scaling_mean': True,\n",
       " 'scaling_var': True,\n",
       " 'scaling_axis': 1,\n",
       " 'hrf': 'spm',\n",
       " 'offset_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/onsets-offsets/',\n",
       " 'duration_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/',\n",
       " 'temporal_shifting': 0,\n",
       " 'base': 10.0,\n",
       " 'voxel_wise': True,\n",
       " 'alpha_percentile': 99.9,\n",
       " 'alpha': None,\n",
       " 'alpha_min_log_scale': 2,\n",
       " 'alpha_max_log_scale': 5,\n",
       " 'nb_alphas': 10,\n",
       " 'optimizing_criteria': 'R2',\n",
       " 'estimator_model': 'Ridge()',\n",
       " 'atlas': 'cort-prob-2mm',\n",
       " 'masker_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english',\n",
       " 'smoothed_masker_path': '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english',\n",
       " 'models': None,\n",
       " 'model_name': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
