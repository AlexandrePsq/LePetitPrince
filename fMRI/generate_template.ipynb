{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we generate the yaml templates that will serve as inputs for the many instanciation of our pipeline.\n",
    "All the templates are saved in the same folder (before being givne as input).\n",
    "When the pipeline has finished running for a given template, this very template is saved in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ap259944/anaconda3/envs/parietal/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/ap259944/anaconda3/envs/parietal/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ap259944/anaconda3/envs/parietal/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ap259944/.local/lib/python3.7/site-packages/nilearn/plotting/cm.py:159: MatplotlibDeprecationWarning: \n",
      "The revcmap function was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use Colormap.reversed() instead.\n",
      "  _cmaps_data[_cmapname_r] = _cm.revcmap(_cmapspec)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from utils import check_folder, read_yaml, save_yaml, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_main = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/code/fMRI/main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dict = {'english': [57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
    "                    72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93,\n",
    "                    94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 113, 114, 115],\n",
    "                'french':[1]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_list = [\n",
    "    'spm', # hrf model used in SPM\n",
    "    'spm + derivative', # SPM model plus its time derivative (2 regressors)\n",
    "    'spm + derivative + dispersion', # idem, plus dispersion derivative (3 regressors)\n",
    "    'glover', # this one corresponds to the Glover hrf\n",
    "    'glover + derivative', # the Glover hrf + time derivative (2 regressors)\n",
    "    'glover + derivative + dispersion' # idem + dispersion derivative\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "    'tr': 2.,\n",
    "    'nb_runs': 9,\n",
    "    'nb_runs_test': 1,\n",
    "    'subject': None,\n",
    "    'scaling_mean': True,\n",
    "    'scaling_var': True,\n",
    "    'parallel': False,\n",
    "    'cuda': True,\n",
    "    'hrf': None,\n",
    "    'voxel_wise': True,\n",
    "    'atlas': 'cort-prob-2mm',\n",
    "    'seed': 1111,\n",
    "    'alpha_percentile': 99.9,\n",
    "    'alpha': None,\n",
    "    'alpha_min_log_scale': 2,\n",
    "    'alpha_max_log_scale': 5,\n",
    "    'nb_alphas': 10,\n",
    "    'optimizing_criteria': 'R2',\n",
    "    'encoding_model': 'Ridge()',\n",
    "    'masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english\",\n",
    "    'smoothed_masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english\",\n",
    "    'path_to_root': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "    'path_to_fmridata': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI\",\n",
    "    'offset_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/wave/english/onsets-offsets\",\n",
    "    'duration_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "    'language': None,\n",
    "    'output': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/\", # Path to the output folder\n",
    "    'input': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/\", # Path to the folder containing the representations\n",
    "    'models': [],\n",
    "    'model_name': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_template(\n",
    "    model_name, \n",
    "    layers, \n",
    "    hidden_states, \n",
    "    attention_heads, \n",
    "    heads,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type,\n",
    "    duration_type,\n",
    "    shift_surprisal,\n",
    "    input_template='activations'):\n",
    "    \n",
    "    columns_to_retrieve = []\n",
    "    if hidden_states:\n",
    "        columns_to_retrieve = ['hidden_state-layer-{}-{}'.format(layer, i) for layer in layers for i in range(1, 769)]\n",
    "    if attention_heads:\n",
    "        columns_to_retrieve += ['attention-layer-{}-head-{}-{}'.format(layer, head, i) for layer in layers for head in heads for i in range(1, 65)]\n",
    "    result = { 'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal,\n",
    "        'input_template': input_template # cls / sep / activations\n",
    "      }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/shell_commands/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n",
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining best hrf kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we determine the best hrf kernel to use with the Bert-base-cased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/jobs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = [57, 63, 67, 77] # identified correct subjects\n",
    "model_name = 'bert-base-cased'\n",
    "hidden_layer_list = [[1], [6], [8], [9], [10], [12]]\n",
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for subject in subject_list:\n",
    "    for hrf in hrf_list:\n",
    "        template['subject'] = subject\n",
    "        template['hrf'] = hrf\n",
    "        hrf = hrf.replace(' ', '')\n",
    "        # hidden-states analysis\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                    False, None, \"Bert_hidden-layer-{}\".format(hidden_layers),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_{}_hidden-layer-{}'.format(hrf, subject, model_name, hidden_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_{}_hidden-layer-{}.yml'.format(hrf, subject, model_name, hidden_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_{}_hidden-layer-{}.sh'.format(hrf, subject, model_name, hidden_layers[0])))\n",
    "        \n",
    "        # Attention analysis\n",
    "        for (layer, head) in attention_layer_head_list:\n",
    "            model = get_model_template(model_name, [layer], False,\n",
    "                                    True, [head], \"Bert_attention-layer-{}_head-{}\".format(layer, head),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_{}_attention-layer-{}_head-{}'.format(hrf, subject, model_name, layer, head)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_{}_attention-layer-{}_head-{}.yml'.format(hrf, subject, model_name, layer, head))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_{}_attention-layer-{}_head-{}.sh'.format(hrf, subject, model_name, layer, head)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that the 'spm + derivative' works best but is almost identical to 'spm' alone. So, in order not to have huge design-matrix, I choose to use 'spm'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Bert bucket versus Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we use Bert-base-cased model and fit a regression model for each layer hidden-states or attention heads for classical sentence-level prediction and sequential prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/jobs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['bert-base-cased', 'bert-base-cased_sequential']\n",
    "hidden_layer_list = [[i] for i in range(13)]\n",
    "attention_layer_list = [[i] for i in range(1, 13)]\n",
    "heads = np.arange(1, 13)\n",
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        # hidden layers comparison\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                            False, None, \"{}_hidden-layer-{}\".format(model_name, hidden_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_hidden-layer-{}'.format(model_name, subject, hidden_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_hidden-layer-{}.yml'.format(model_name, subject, hidden_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_hidden-layer-{}.sh'.format(model_name, subject, hidden_layers[0])))\n",
    "        \n",
    "        # attention layers comparison\n",
    "        for attention_layers in attention_layer_list:\n",
    "            model = get_model_template(model_name, attention_layers, False, \n",
    "                                            True, heads, \"{}_attention-layer-{}\".format(model_name, attention_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention_layer-{}'.format(model_name, subject, attention_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}.yml'.format(model_name, subject, attention_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}.sh'.format(model_name, subject, attention_layers[0])))\n",
    "            \n",
    "        # specific attention heads comparison    \n",
    "        for (layer, head) in attention_layer_head_list:\n",
    "            model = get_model_template(model_name, [layer], False,\n",
    "                                    True, [head], \"Bert_attention-layer-{}_head-{}\".format(layer, head),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention-layer-{}_head-{}'.format(model_name, subject, layer, head)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}_head-{}.yml'.format(model_name, subject, layer, head))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}_head-{}.sh'.format(model_name, subject, layer, head)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3366"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(command_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention heads analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we look at the regression model resulting from specific attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already included in the first comparison..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of fine-tuned Bert models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we fit each layer hidden-states (and potentially specific heads attention) of each fine-tuned Bert model to the fMRI data of all english subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert VS Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/jobs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['roberta-base']\n",
    "hidden_layer_list = [[i] for i in range(13)]\n",
    "attention_layer_list = [[i] for i in range(1, 13)]\n",
    "heads = np.arange(1, 13)\n",
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        # hidden layers comparison\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                            False, None, \"{}_hidden-layer-{}\".format(model_name, hidden_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_hidden-layer-{}'.format(model_name, subject, hidden_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_hidden-layer-{}.yml'.format(model_name, subject, hidden_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_hidden-layer-{}.sh'.format(model_name, subject, hidden_layers[0])))\n",
    "        \n",
    "        # attention layers comparison\n",
    "        for attention_layers in attention_layer_list:\n",
    "            model = get_model_template(model_name, attention_layers, False, \n",
    "                                            True, heads, \"{}_attention-layer-{}\".format(model_name, attention_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention_layer-{}'.format(model_name, subject, attention_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}.yml'.format(model_name, subject, attention_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}.sh'.format(model_name, subject, attention_layers[0])))\n",
    "            \n",
    "        # specific attention heads comparison    \n",
    "        for (layer, head) in attention_layer_head_list:\n",
    "            model = get_model_template(model_name, [layer], False,\n",
    "                                    True, [head], \"Bert_attention-layer-{}_head-{}\".format(layer, head),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention-layer-{}_head-{}'.format(model_name, subject, layer, head)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}_head-{}.yml'.format(model_name, subject, layer, head))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}_head-{}.sh'.format(model_name, subject, layer, head)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(command_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/jobs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['bert-base-cased']\n",
    "hidden_layer_list = [[i for i in range(13)]]\n",
    "attention_layer_list = [[i for i in range(1, 13)]]\n",
    "heads = np.arange(1, 13)\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        # hidden layers comparison\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                            False, None, \"{}_all-hidden-layers\".format(model_name),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_all-hidden-layers'.format(model_name, subject)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_all-hidden-layers.yml'.format(model_name, subject))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_all-hidden-layers.sh'.format(model_name, subject)))\n",
    "        \n",
    "        # attention layers comparison\n",
    "        for attention_layers in attention_layer_list:\n",
    "            model = get_model_template(model_name, attention_layers, False, \n",
    "                                            True, heads, \"{}_all-attention-layers\".format(model_name),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_all-attention-layers'.format(model_name, subject)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_all-attention-layers.yml'.format(model_name, subject))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_all-attention-layers.sh'.format(model_name, subject)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_control/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_control/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_control/jobs.txt\"\n",
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n",
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix\n",
    "seeds = [24, 213, 1111, 61, 183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_names = ['bert-base-cased_control_{}_layer-{}'.format(seed, layer) for seed in seeds for layer in range(13)]\n",
    "hidden_layer_list = [[i] for i in range(13)]\n",
    "attention_layer_list = [[i] for i in range(1, 13)]\n",
    "heads = np.arange(1, 13)\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "\n",
    "        # hidden layers comparison\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model_name = 'bert-base-cased_control_{}_layer-{}'.format(seed, hidden_layers[0])\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                            False, None, \"{}_hidden-layer-{}\".format(model_name, hidden_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_hidden-layer-{}'.format(model_name, subject, hidden_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_hidden-layer-{}.yml'.format(model_name, subject, hidden_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_hidden-layer-{}.sh'.format(model_name, subject, hidden_layers[0])))\n",
    "\n",
    "        # attention layers comparison\n",
    "        for attention_layers in attention_layer_list:\n",
    "            model_name = 'bert-base-cased_control_{}_layer-{}'.format(seed, attention_layers[0])\n",
    "            model = get_model_template(model_name, attention_layers, False, \n",
    "                                            True, heads, \"{}_attention-layer-{}\".format(model_name, attention_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention_layer-{}'.format(model_name, subject, attention_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}.yml'.format(model_name, subject, attention_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}.sh'.format(model_name, subject, attention_layers[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_gpt2/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_gpt2/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_gpt2/jobs.txt\"\n",
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n",
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix\n",
    "seeds = [24, 213, 1111, 61, 183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['gpt2']\n",
    "hidden_layer_list = [[i] for i in range(13)]\n",
    "attention_layer_list = [[i] for i in range(1, 13)]\n",
    "heads = np.arange(1, 13)\n",
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        # hidden layers comparison\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                            False, None, \"{}_hidden-layer-{}\".format(model_name, hidden_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_hidden-layer-{}'.format(model_name, subject, hidden_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_hidden-layer-{}.yml'.format(model_name, subject, hidden_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_hidden-layer-{}.sh'.format(model_name, subject, hidden_layers[0])))\n",
    "        \n",
    "        # attention layers comparison\n",
    "        for attention_layers in attention_layer_list:\n",
    "            model = get_model_template(model_name, attention_layers, False, \n",
    "                                            True, heads, \"{}_attention-layer-{}\".format(model_name, attention_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention_layer-{}'.format(model_name, subject, attention_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}.yml'.format(model_name, subject, attention_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}.sh'.format(model_name, subject, attention_layers[0])))\n",
    "            \n",
    "        # specific attention heads comparison    \n",
    "        for (layer, head) in attention_layer_head_list:\n",
    "            model = get_model_template(model_name, [layer], False,\n",
    "                                    True, [head], \"Bert_attention-layer-{}_head-{}\".format(layer, head),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention-layer-{}_head-{}'.format(model_name, subject, layer, head)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}_head-{}.yml'.format(model_name, subject, layer, head))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}_head-{}.sh'.format(model_name, subject, layer, head)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_template(\n",
    "    model_name, \n",
    "    layers, \n",
    "    ninp,\n",
    "    nhid,\n",
    "    parameters,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type,\n",
    "    duration_type,\n",
    "    shift_surprisal,\n",
    "    includ_surprisal,\n",
    "    includ_entropy,\n",
    "    input_template='activations'):\n",
    "    \n",
    "    columns_to_retrieve = []\n",
    "    for param in parameters:\n",
    "        columns_to_retrieve = ['{}-layer-{}-{}'.format(param, layer, i) for layer in layers for i in range(1, nhid + 1)]\n",
    "    if includ_surprisal:\n",
    "        columns_to_retrieve += ['surprisal']\n",
    "    if includ_entropy:\n",
    "        columns_to_retrieve += ['entropy']\n",
    "    result = { 'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal,\n",
    "        'input_template': input_template # activations\n",
    "      }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/jobs.txt\"\n",
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n",
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_lstm/jobs.txt\"\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['LSTM_embedding-size_650_nhid_650_nlayers_2_dropout_02_wiki_kristina_english',\n",
    " 'LSTM_embedding-size_600_nhid_300_nlayers_1_dropout_02_wiki_kristina_english']\n",
    "hidden_layer_list = [[i for i in range(1, 3)], [1]]\n",
    "parameters_list = [['in'], ['forget'], ['out'], ['c_tilde'], ['cell']] #['hidden'],\n",
    "data_compression = ['pca', None]\n",
    "ncomponents = [300, None]\n",
    "shift_surprisal = False\n",
    "includ_surprisal = False\n",
    "includ_entropy = False\n",
    "params = [{'ninp': 650, 'nhid': 650, 'nlayers': 2}, {'ninp': 600, 'nhid': 300, 'nlayers': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, model_name in enumerate(model_names):\n",
    "    param = params[index]\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        for parameters in parameters_list:\n",
    "        \n",
    "            # hidden layers comparison\n",
    "            model = get_lstm_template(model_name, \n",
    "                                        hidden_layer_list[index], \n",
    "                                        param['ninp'],\n",
    "                                        param['nhid'],\n",
    "                                        parameters,\n",
    "                                        \"{}_all-{}-layers\".format(model_name, parameters[0]),\n",
    "                                        data_compression[index], \n",
    "                                        ncomponents[index],\n",
    "                                        \"word+punctuation\",\n",
    "                                        None,\n",
    "                                        shift_surprisal,\n",
    "                                        includ_surprisal,\n",
    "                                        includ_entropy,\n",
    "                                        input_template='activations')\n",
    "\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_all-{}-layers'.format(model_name, subject, parameters[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_all-{}-layers.yml'.format(model_name, subject, parameters[0]))\n",
    "\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_all-{}-layers.sh'.format(model_name, subject, parameters[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_template(\n",
    "    model_name, \n",
    "    embedding_size,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type,\n",
    "    duration_type,\n",
    "    input_template='activations'):\n",
    "    \n",
    "    columns_to_retrieve = ['embedding-{}'.format(i) for i in range(1, embedding_size + 1)]\n",
    "    result = { 'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal,\n",
    "        'input_template': input_template # activations\n",
    "      }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_glove/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_glove/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_glove/jobs.txt\"\n",
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n",
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_glove/jobs.txt\"\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['glove_embeddings']\n",
    "embedding_size = [300]\n",
    "data_compression = [None]\n",
    "ncomponents = [None]\n",
    "shift_surprisal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_name in enumerate(model_names):\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        # hidden layers comparison\n",
    "        model = get_glove_template(model_name, \n",
    "                                    embedding_size[index],\n",
    "                                    'glove_{}'.format(embedding_size[index]),\n",
    "                                    data_compression[index], \n",
    "                                    ncomponents[index],\n",
    "                                    \"word+punctuation\",\n",
    "                                    None,\n",
    "                                    input_template='activations')\n",
    "\n",
    "        template['models'] = [model]\n",
    "        template['model_name'] =  'glove_{}_{}'.format(embedding_size[index], subject)\n",
    "        yaml_path = os.path.join(templates_folder, 'glove_{}_{}.yml'.format(embedding_size[index], subject))\n",
    "\n",
    "        save_yaml(template, yaml_path)\n",
    "        command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "        path_to_sh.append(os.path.join(sh_folder, 'glove_{}_{}.sh'.format(embedding_size[index], subject)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BF_template(\n",
    "    model_name, \n",
    "    column_names,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type,\n",
    "    duration_type,\n",
    "    shift_surprisal,\n",
    "    input_template='activations'):\n",
    "    \n",
    "    columns_to_retrieve = column_names\n",
    "    result = { 'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal,\n",
    "        'input_template': input_template # activations\n",
    "      }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_BF/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_BF/shell_commands/\"\n",
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_BF/jobs.txt\"\n",
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n",
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp_BF/jobs.txt\"\n",
    "command_lines = []\n",
    "path_to_sh = []\n",
    "hrf = 'spm' # 'spm + derivative' slightly better but not so much as to double the design-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['log_frequency', 'rms']\n",
    "data_compression = [None, None]\n",
    "ncomponents = [None, None]\n",
    "shift_surprisal = [False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, model_name in enumerate(model_names):\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        # hidden layers comparison\n",
    "        model = get_BF_template(model_name, \n",
    "                                    [model_name],\n",
    "                                    model_name,\n",
    "                                    data_compression[index], \n",
    "                                    ncomponents[index],\n",
    "                                    \"word\",\n",
    "                                    None,\n",
    "                                    shift_surprisal[index],\n",
    "                                    input_template='activations')\n",
    "\n",
    "        template['models'] = [model]\n",
    "        template['model_name'] =  'BF_{}_{}'.format(model_name, subject)\n",
    "        yaml_path = os.path.join(templates_folder, 'BF_{}_{}.yml'.format(model_name, subject))\n",
    "\n",
    "        save_yaml(template, yaml_path)\n",
    "        command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "        path_to_sh.append(os.path.join(sh_folder, 'BF_{}_{}.sh'.format(model_name, subject)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
