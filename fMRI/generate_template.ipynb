{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we generate the yaml templates that will serve as inputs for the many instanciation of our pipeline.\n",
    "All the templates are saved in the same folder (before being givne as input).\n",
    "When the pipeline has finished running for a given template, this very template is saved in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from utils import check_folder, read_yaml, save_yaml, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_main = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/code/fMRI/main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dict = {'english': [57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
    "                    72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93,\n",
    "                    94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 113, 114, 115],\n",
    "                'french':[1]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_list = [\n",
    "    'spm', # hrf model used in SPM\n",
    "    'spm + derivative', # SPM model plus its time derivative (2 regressors)\n",
    "    'spm + derivative + dispersion', # idem, plus dispersion derivative (3 regressors)\n",
    "    'glover', # this one corresponds to the Glover hrf\n",
    "    'glover + derivative', # the Glover hrf + time derivative (2 regressors)\n",
    "    'glover + derivative + dispersion' # idem + dispersion derivative\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "    'tr': 2.,\n",
    "    'nb_runs': 9,\n",
    "    'nb_runs_test': 1,\n",
    "    'subject': None,\n",
    "    'scaling_mean': True,\n",
    "    'scaling_var': True,\n",
    "    'parallel': False,\n",
    "    'cuda': True,\n",
    "    'hrf': None,\n",
    "    'voxel_wise': True,\n",
    "    'atlas': 'cort-prob-2mm',\n",
    "    'seed': 1111,\n",
    "    'alpha_percentile': 99.9,\n",
    "    'alpha': None,\n",
    "    'alpha_min_log_scale': 2,\n",
    "    'alpha_max_log_scale': 5,\n",
    "    'nb_alphas': 10,\n",
    "    'optimizing_criteria': 'R2',\n",
    "    'encoding_model': 'Ridge()',\n",
    "    'masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english\",\n",
    "    'smoothed_masker_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english\",\n",
    "    'path_to_root': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "    'path_to_fmridata': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/fMRI\",\n",
    "    'offset_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/wave/english/onsets-offsets\",\n",
    "    'duration_path': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/\",\n",
    "    'language': None,\n",
    "    'output': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/maps/\", # Path to the output folder\n",
    "    'input': \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/\", # Path to the folder containing the representations\n",
    "    'models': [],\n",
    "    'model_name': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_template(\n",
    "    model_name, \n",
    "    layers, \n",
    "    hidden_states, \n",
    "    attention_heads, \n",
    "    heads,\n",
    "    surname,\n",
    "    data_compression, \n",
    "    ncomponents,\n",
    "    offset_type,\n",
    "    duration_type,\n",
    "    shift_surprisal):\n",
    "    \n",
    "    columns_to_retrieve = []\n",
    "    if hidden_states:\n",
    "        columns_to_retrieve = ['hidden_state-layer-{}-{}'.format(layer, i) for layer in layers for i in range(1, 769)]\n",
    "    if attention_heads:\n",
    "        columns_to_retrieve += ['attention-layer-{}-head-{}-{}'.format(layer, head, i) for layer in layers for head in heads for i in range(1, 65)]\n",
    "    result = { 'model_name': model_name,\n",
    "        'columns_to_retrieve': str(columns_to_retrieve),\n",
    "        'surname': surname,\n",
    "        'data_compression': data_compression,\n",
    "        'ncomponents': ncomponents,\n",
    "        'offset_type': offset_type, # word / word+punctuation / ...,\n",
    "        'duration_type': duration_type,\n",
    "        'shift_surprisal': shift_surprisal\n",
    "      }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "templates_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/templates/\"\n",
    "sh_folder = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/shell_commands/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_folder(templates_folder)\n",
    "check_folder(sh_folder)\n",
    "template['language'] = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining best hrf kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we determine the best hrf kernel to use with the Bert-base-cased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/jobs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = [57, 63, 67, 77] # identified correct subjects\n",
    "model_name = 'bert-base-cased'\n",
    "hidden_layer_list = [[1], [6], [8], [9], [10], [12]]\n",
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for subject in subject_list:\n",
    "    for hrf in hrf_list:\n",
    "        template['subject'] = subject\n",
    "        template['hrf'] = hrf\n",
    "        hrf = hrf.replace(' ', '')\n",
    "        # hidden-states analysis\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                    False, None, \"Bert_hidden-layer-{}\".format(hidden_layers),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_{}_hidden-layer-{}'.format(hrf, subject, model_name, hidden_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_{}_hidden-layer-{}.yml'.format(hrf, subject, model_name, hidden_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_{}_hidden-layer-{}.sh'.format(hrf, subject, model_name, hidden_layers[0])))\n",
    "        \n",
    "        # Attention analysis\n",
    "        for (layer, head) in attention_layer_head_list:\n",
    "            model = get_model_template(model_name, [layer], False,\n",
    "                                    True, [head], \"Bert_attention-layer-{}_head-{}\".format(layer, head),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_{}_attention-layer-{}_head-{}'.format(hrf, subject, model_name, layer, head)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_{}_attention-layer-{}_head-{}.yml'.format(hrf, subject, model_name, layer, head))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_{}_attention-layer-{}_head-{}.sh'.format(hrf, subject, model_name, layer, head)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that the 'spm + derivative' works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Bert bucket versus Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we use Bert-base-cased model and fit a regression model for each layer hidden-states or attention heads for classical sentence-level prediction and sequential prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_launch_path = \"/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/tmp/jobs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = 'spm + derivative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['bert-base-cased', 'bert-base-cased_sequential']\n",
    "hidden_layer_list = [[i] for i in range(13)]\n",
    "attention_layer_list = [[i] for i in range(1, 13)]\n",
    "heads = np.arange(1, 13)\n",
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]\n",
    "command_lines = []\n",
    "path_to_sh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    for subject in subject_dict[language]:\n",
    "        template['hrf'] = hrf\n",
    "        template['subject'] = subject\n",
    "        \n",
    "        # hidden layers comparison\n",
    "        for hidden_layers in hidden_layer_list:\n",
    "            model = get_model_template(model_name, hidden_layers, True, \n",
    "                                            False, None, \"{}_hidden-layer-{}\".format(model_name, hidden_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_hidden-layer-{}'.format(model_name, subject, hidden_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_hidden-layer-{}.yml'.format(model_name, subject, hidden_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_hidden-layer-{}.sh'.format(model_name, subject, hidden_layers[0])))\n",
    "        \n",
    "        # attention layers comparison\n",
    "        for attention_layers in attention_layer_list:\n",
    "            model = get_model_template(model_name, attention_layers, False, \n",
    "                                            True, heads, \"{}_attention-layer-{}\".format(model_name, attention_layers),\n",
    "                                            None, None,\n",
    "                                            \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention_layer-{}'.format(model_name, subject, attention_layers[0])\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}.yml'.format(model_name, subject, attention_layers[0]))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}.sh'.format(model_name, subject, attention_layers[0])))\n",
    "            \n",
    "        # specific attention heads comparison    \n",
    "        for (layer, head) in attention_layer_head_list:\n",
    "            model = get_model_template(model_name, [layer], False,\n",
    "                                    True, [head], \"Bert_attention-layer-{}_head-{}\".format(layer, head),\n",
    "                                    None, None,\n",
    "                                    \"word+punctuation\", None, False)\n",
    "            template['models'] = [model]\n",
    "            template['model_name'] = '{}_{}_attention-layer-{}_head-{}'.format(model_name, subject, layer, head)\n",
    "            yaml_path = os.path.join(templates_folder, '{}_{}_attention-layer-{}_head-{}.yml'.format(model_name, subject, layer, head))\n",
    "            save_yaml(template, yaml_path)\n",
    "            command_lines.append(\"python {} --yaml_file {}\".format(path_to_main, yaml_path))\n",
    "            path_to_sh.append(os.path.join(sh_folder, '{}_{}_attention-layer-{}_head-{}.sh'.format(model_name, subject, layer, head)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, command in enumerate(command_lines):\n",
    "    write(path_to_sh[index], command)\n",
    "    queue = 'Nspin_long' # 'Nspin_bigM'\n",
    "    walltime = '99:00:00'\n",
    "    output_log = '/home/ap259944/logs/log_o_{}'.format(index)\n",
    "    error_log = '/home/ap259944/logs/log_e_{}'.format(index)\n",
    "    job_name = os.path.basename(path_to_sh[index]).split('.')[0]\n",
    "    write(job_to_launch_path, f\"qsub -q {queue} -N {job_name} -l walltime={walltime} -o {output_log} -e {error_log} {path_to_sh[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3366"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(command_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention heads analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we look at the regression model resulting from specific attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention_layer_head_list = [[7, 6], [4, 10], [8, 1], [8,2], [6,7], [8, 10], [8, 11], [9, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already included in the first comparison..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of fine-tuned Bert models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we fit each layer hidden-states (and potentially specific heads attention) of each fine-tuned Bert model to the fMRI data of all english subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
