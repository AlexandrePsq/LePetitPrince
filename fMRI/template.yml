tr: 2.
nb_runs: 9
subject: 57
scaling_mean: True
scaling_var: True
parallel: False
cuda: True
hrf: spm
voxel_wise: True
atlas: cort-prob-2mm
seed: 1111
alpha_percentile: 99.9
alpha_min_log_scale: 2
alpha_max_log_scale: 5
nb_alphas: 25
masker_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english"
smoothed_masker_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english"
path_to_root: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/"
path_to_fmridata: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/"
offset_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/wave"
duration_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/"
language: english
models:
  - model_name: wordrate_all_model
    columns_to_retrieve: "[0]"
    surname: Word_rate
    data_compression: 
    ncomponents: 
    offset_type: 'word' # word / word+punctuation / ...
    duration_type: 
    shift_surprisal: False
  - model_name: wordrate_all_model
    columns_to_retrieve: "[3]"
    surname: unigram
    data_compression: 
    ncomponents: 
    offset_type: 'word' # word / word+punctuation / ...
    duration_type: 
    shift_surprisal: False
model_name: Word_rate+unigram
#models:
#  - model_name: bert_all-layers
#    columns_to_retrieve: "[i for i in range(0,500)]" #example 
#    surname: BERT-small
#    data_compression: pca
#    ncomponents: 300
#    offset_type: 'word' # word / word+punctuation / ...
#    duration_type: None
#    shift_surprisal: False
#  - model_name: wordrate
#    columns_to_retrieve: "[0]"
#    surname: Word_rate
#    data_compression: None
#    ncomponents: None
#    offset_type: 'word' # word / word+punctuation / ...
#    duration_type: None
#    shift_surprisal: False
#model_name: BERT-small+Word_rate
