tr: 2.
nb_runs: 9
subject: 57
scaling_mean: True
scaling_var: True
parallel: False
cuda: True
voxel_wise: True
atlas: cort-prob-2mm
seed: 1111
alpha_percentile: 99.9
alpha_min_log_scale: 2
alpha_max_log_scale: 5
nb_alphas: 25
masker_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english.nii.gz"
smoothed_masker_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/smoothed_global_masker_english.nii.gz"
path_to_root: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/"
path_to_fmridata: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/"
offset_path: ""
language: english
models:
  - model_name: bert_all-layers
    columns_to_retrieve: "np.array([i for i in range(0,500)])" #example # MUST BE a np.array !!
    surname: BERT-small
    data_compression: pca
    ncomponents: 300
    offset_type: 'word' # word / word+punctuation / ...
    duration_type: None
    shift_surprisal: False
  - model_name: wordrate
    columns_to_retrieve: "[0]"
    surname: Word_rate
    data_compression: None
    ncomponents: None
    offset_type: 'word' # word / word+punctuation / ...
    duration_type: None
    shift_surprisal: False
model_name: BERT-small+Word_rate
