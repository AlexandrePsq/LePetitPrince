tr: 2.
nb_runs: 9
subject: 57
scaling_mean: True
scaling_var: True
parallel: False
cuda: True
voxel_wise: True
atlas: cort-prob-2mm
seed: 1111
alpha_percentile: 99.9
alpha_min_log_scale: 2
alpha_max_log_scale: 5
nb_alphas: 25
masker_path: "/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/global_masker_english.nii.gz"
models:
  - model_name: bert_all-layers
    columns_to_retrieve: "[i for i in range(0,50)]" #example
    surname: BERT-small
    language: english
    data_compression: None
    compression_kwargs: None
    onset_type: 'word' # word / word+punctuation / ...
    shift_surprisal: False
  - model_name: wordrate
    columns_to_retrieve: None
    surname: Word_rate
    language: english
    data_compression: None
    compression_kwargs: None
    onset_type: 'word' # word / word+punctuation / ...
    shift_surprisal: False
model_name: BERT-small+Word_rate
